<!DOCTYPE html>
<!-- saved from url=(0027)https://rtqichen.github.io/ -->
<html lang="en"><head><meta http-equiv="Content-Type" content="text/html; charset=UTF-8">

  
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <meta name="viewport" content="width=device-width, initial-scale=1">

  <title>Khashayar Gatmiry</title>
  <meta name="description" content="">

  
  <link rel="stylesheet" href="./Ricky T. Q. Chen_files/main.css">
  <link rel="canonical" href="https://rtqichen.github.io/">
  <link rel="alternate" type="application/rss+xml" title="Ricky T. Q. Chen" href="https://rtqichen.github.io/feed.xml">

</head>

<body>

  <header class="site-header" role="banner">
    <div class="wrapper navigation-wrapper ">
      <div class="navigation-links">
        <span class="site-title">Khashayar Gatmiry</span>

      </div>
    </div>
  </header>

  <main class="page-content" aria-label="Content">
    <div class="wrapper">

      <!-- Intro -->
      <article class="post">
        <div class="post-content">

          <img src="./Ricky T. Q. Chen_files/myheadshot.jpeg" class="profile-picture">

          <br>
          <p style="font-family: palatino;">
            Hi! I'm a final-year graduate student at MIT, EECS, and CSAIL. I am very fortunate to be co-advised by Stefanie Jegelka and Jonathan Kelner.
            My research interests are broadly machine learning theory, deep learning, sampling, and optimization.
          </p>

          <p style="font-family: palatino;">
            In Fall 2023 I was a research intern at Google, NYC, in the BigML group, under the wise supervision of Sashank Reddi and Sobhan Miryouseffi, where I was exploringmethods to 
            improve the training time of the Bert model. Last summer I was a research 
            intern at Microsoft in the Foundation of Machine learning group, exploring the strange capabilities of small language models for building a Euclidean embedding 
            for short stories. At microsoft, I was fortunate to be Advised by Ronen Eldan, Adil Salim, and Yi Zhang.
          </p>
          <p>
            
          </p>
          <p>
            <a href="https://gatmiry.github.io/pdfs/CV.pdf">CV</a> |
            <!--a href="https://github.com/rtqichen">Github</a--> 
            <!--<a href="https://twitter.com/RickyTQChen">Twitter</a> |-->
            <a href="https://scholar.google.com/citations?user=w8XocYQAAAAJ&hl=en">Google Scholar</a> |
            <a href="mailto:gatmiry@mit.edu">gatmiry@mit.edu</a>
          </p>
          <p>
            <!--a href="https://rtqichen.github.io/index.html">Research</a--> 
            <!--a href="https://rtqichen.github.io/slides/index.html">Talk slides</a--> 
          </p>

        </div>
      </article>
    </div>
    <div class="wrapper">

      <!-- Research -->
      <article class="post">
        <header class="post-header">
          <h1 class="post-title">Publications (by topic)</h1>
          <!--(<a href="https://scholar.google.com/citations?user=w8XocYQAAAAJ&hl=en">Google Scholar</a>)-->
        </header>

       

        <div class="post-content">
          <ul class="publications">

            <!--h1 class="post-subtitle">Deep learning</h1-->

<h1 class="post-subsubtitle">Transformers</h1>
            
  

<li class="article">
  <span class="title">
    On the Role of Depth and Looping for In-Context Learning with Task Diversity
  </span>
  <span class="authors">
      <strong> Khashayar Gatmiry </strong>, Zhiyuan Li, Sashank J. Reddi, Stefanie Jegelka
  </span>
  <span class="journal-info">Preprint.</span>
  <!--span class="year">2024</span-->
  <span class="links">
      <!--a href="https://proceedings.mlr.press/v235/gatmiry24a.html">[paper]</a-->
  </span>
</li>

<li class="article">
    <span class="title">
        Can Looped Transformers Learn to Implement Multi-step Gradient Descent for In-context Learning?
    </span>
    <span class="authors">
      <strong>Khashayar Gatmiry</strong>, Nikunj Saunshi, Sashank J Reddi, Stefanie Jegelka, Sanjiv Kumar
    </span>
    <span class="journal-info">ICML 2024</span>
    <!--span class="year">2024</span-->
    <span class="links">
        <a href="https://arxiv.org/abs/2410.08292">[paper]</a>
    </span>
  </li>

  <li class="article">
    <span class="title">
      Rethinking Invariance in In-context Learning
    </span>
    <span class="authors">
     Lizhe Fang, Yifei Wang, <strong>Khashayar Gatmiry</strong>, Lei Fang, Yisen Wang
    </span>
    <span class="journal-info">ICML Workshop 2024</span>
    <!--span class="year">2024</span-->
    <span class="links">
        <a href="https://openreview.net/pdf?id=xSDqxxILWg">[paper]</a>
    </span>
  </li>

  
      <h1 class="post-subsubtitle">Implicit Bias</h1>

  <li class="article">
    <span class="title">
        Simplicity Bias via Global Convergence of Sharpness Minimization
    </span>
    <span class="authors">
        <strong> Khashayar Gatmiry </strong>, Zhiyuan Li, Sashank J. Reddi, Stefanie Jegelka
    </span>
    <span class="journal-info">ICML 2024</span>
    <!--span class="year">2024</span-->
    <span class="links">
        <a href="https://proceedings.mlr.press/v235/gatmiry24a.html">[paper]</a>
    </span>
  </li>

            
            
  <li class="article">
    <span class="title">
        The Inductive Bias of Flatness Regularization for Deep Matrix Factorization
    </span>
    <span class="authors">
      <strong>Khashayar Gatmiry</strong>, Zhiyuan Li, Ching-Yao Chuang, Sashank Reddi, Tengyu Ma, Stefanie Jegelka
    </span>
    <span class="journal-info">NeuRIPS 2023</span>
    <!--span class="year">2023</span-->
    <span class="links">
        <a href="https://arxiv.org/abs/2306.13239">[paper]</a>
    </span>
  </li>

  <li class="article">
    <span class="title">
        The Inductive Bias of Flatness Regularization for Deep Matrix Factorization
    </span>
    <span class="authors">
      <strong>Khashayar Gatmiry</strong>, Zhiyuan Li, Ching-Yao Chuang, Sashank Reddi, Tengyu Ma, Stefanie Jegelka
    </span>
    <span class="journal-info">NeuRIPS 2023</span>
    <!--span class="year">2023</span-->
    <span class="links">
        <a href="https://arxiv.org/abs/2306.13239">[paper]</a>
    </span>
  </li>


            <h1 class="post-subsubtitle">Algorithmic Generalization and Stability</h1>

<li class="article">
    <span class="title">
        Adaptive Generalization and Optimization of Three-Layer Neural Networks
    </span>
    <span class="authors">
        Khashayar Gatmiry, Stefanie Jegelka, Jonathan Kelner
    </span>
    <span class="journal-info">ICLR 2022</span>
    <!--span class="year">2022</span-->
    <span class="links">
        <a href="https://par.nsf.gov/biblio/10406731-adaptive-generalization-optimization-three-layer-neural-networks">[paper]</a>
    </span>
</li>

<li class="article">
    <span class="title">
        On the generalization of learning algorithms that do not converge
    </span>
    <span class="authors">
        Nisha Chandramoorthy, Andreas Loukas, Khashayar Gatmiry, Stefanie Jegelka
    </span>
    <span class="journal-info">NeurIPS 2022.</span>
    <!--span class="year">2022</span-->
    <span class="links">
        <a href="https://openreview.net/forum?id=4tGggvizjd8">[paper]</a>
    </span>
</li>

            <!--h1 class="post-subtitle">Sampling</h1-->

<h1 class="post-subsubtitle">Diffusion Models</h1>
  <li class="article">
    <span class="title">
        Learning Mixture of Gaussians Using Diffusion Models
    </span>
    <span class="authors">
        <strong>Khashayar Gatmiry</strong>, Jonathan Kelner, Holden Lee
    </span>
    <span class="journal-info">Preprint</span>
    <!--span class="year">2022</span-->
    <span class="links">
        <a href="https://arxiv.org/abs/2404.18869">[paper]</a>
    </span>
</li>
  <li class="article">
    <span class="title">
      What does guidance do? A fine-grained analysis in a simple setting
    </span>
    <span class="authors">
        Muthu Chidambaram, <strong>Khashayar Gatmiry</strong>, Sitan Chen, Holden Lee, Jianfeng Lu
    </span>
    <span class="journal-info">NeuRIPS 2024</span>
    <!--span class="year">2022</span-->
    <span class="links">
        <a href="https://www.arxiv.org/abs/2409.13074">[paper]</a>
    </span>
</li>

<h1 class="post-subsubtitle">MCMC Sampling</h1>
<li class="article">
    <span class="title">
        Sampling Polytopes with Riemannian HMC: Faster Mixing via the Lewis Weights Barrier 
    </span>
    <span class="authors">
        Khashayar Gatmiry, Jonathan Kelner, Santosh S. Vempala
    </span>
    <span class="journal-info">COLT 2024.</span>
    <!--span class="year">2024</span-->
    <span class="links">
        <a href="https://proceedings.mlr.press/v247/gatmiry24a/gatmiry24a.pdf">[paper]</a>
    </span>
</li>

<li class="article">
    <span class="title">
        Convergence of the Riemannian Langevin Algorithm 
    </span>
    <span class="authors">
        Khashayar Gatmiry, Jonathan Kelner, Santosh S. Vempala
    </span>
    <span class="journal-info">to appear in JMLR</span>
    <!--span class="year">2024</span-->
    <span class="links">
        <a href="https://arxiv.org/abs/2204.10818">[paper]</a>
    </span>
</li>

<li class="article">
    <span class="title">
        When does Metropolized Hamiltonian Monte Carlo provably outperform Metropolis-adjusted Langevin algorithm? 
    </span>
    <span class="authors">
        Yuansi Chen, Khashayar Gatmiry
    </span>
    <span class="journal-info"></span>
    <span class="journal-info">Preprint</span>
    <!--span class="year">2023</span-->
    <span class="links">
        <a href="https://arxiv.org/abs/2304.04724">[paper]</a>
    </span>
</li>



            
<h1 class="post-subsubtitle">Online Learning/Optimization</h1>
<li class="article">
    <span class="title">
      Computing Optimal Regularizers for Online Linear Optimization
    </span>
    <span class="authors">
        Khashayar Gatmiry, Jon SChneider, Stefanie Jegelka
    </span>
    <span class="journal-info">Preprint</span>
    <!--span class="year">2024</span-->
    <span class="links">
        <a href="https://arxiv.org/abs/2410.17336">[paper]</a>
    </span>
</li>

<li class="article">
  <span class="title">
    Projection-Free Online Convex Optimization via Efficient Newton Iterations
  </span>
  <span class="authors">
      Khashayar Gatmiry, Zakaria Mhammedi
  </span>
  <span class="journal-info">NeuRIPS 2023</span>
  <!--span class="year">2023</span-->
  <span class="links">
      <a href="https://arxiv.org/abs/2410.17336">[paper]</a>
  </span>
</li>

<li class="article">
  <span class="title">
    Quasi-Newton Steps for Efficient Online Exp-Concave Optimization
  </span>
  <span class="authors">
    Zakaria Mhammedi, Khashayar Gatmiry
  </span>
  <span class="journal-info">COLT 2023</span>
  <!--span class="year">2023</span-->
  <span class="links">
      <a href="https://proceedings.mlr.press/v195/mhammedi23a.html">[paper]</a>
  </span>
</li>

<li class="article">
  <span class="title">
    Quasi-Newton Steps for Efficient Online Exp-Concave Optimization
  </span>
  <span class="authors">
    Khashayar Gatmiry, Jon Schneider
  </span>
  <span class="journal-info">COLT 2024</span>
  <!--span class="year">2024</span-->
  <span class="links">
      <a href="https://arxiv.org/abs/2407.00571">[paper]</a>
  </span>
</li>

<li class="article">
  <span class="title">
    
  </span>
  <span class="authors">
    Khashayar Gatmiry, Thomas Kesselheim, Sahil Singla, Yifan Wang
  </span>
  <span class="journal-info">SODA 2024</span>
  <!--span class="year">2024</span-->
  <span class="links">
      <a href="https://arxiv.org/abs/2211.08586">[paper]</a>
  </span>
</li>

  
<h1 class="post-subsubtitle">Other Testing/Learning</h1>
<li class="article">
  <span class="title">
   EM for Mixture of Linear Regression with Clustered Data
  </span>
  <span class="authors">
      Amirhossein Reisizadeh, <strong>Khashayar Gatmiry</strong>, Asuman Ozdaglar 
  </span>
  <span class="journal-info">AISTATS 2024</span>
  <!--span class="year">2024</span-->
  <span class="links">
      <a href="https://proceedings.mlr.press/v238/reisizadeh24a.html">[paper]</a>
  </span>
</li>

<li class="article">
  <span class="title">
    Testing Determinantal Point Processes
  </span>
  <span class="authors">
      <strong>Khashayar Gatmiry</strong>, Maryam Aliakbarpour, Stefanie Jegelka
  </span>
  <span class="journal-info">NeuRIPS 2020</span>
  <!--span class="year">2024</span-->
  <span class="links">
      <a href="https://arxiv.org/abs/2008.03650">[paper]</a>
  </span>
</li>

<li class="article">
    <span class="title">
      A Unified Approach to Controlling Implicit Regularization via Mirror Descent
    </span>
    <span class="authors">
        Haoyuan Sun, <strong>Khashayar Gatmiry</strong>, Kwangjun Ahn, Navid Azizan
    </span>
    <span class="journal-info">JMLR 2023</span>
    <!--span class="year">2024</span-->
    <span class="links">
        <a href="https://arxiv.org/abs/2306.13853">[paper]</a>
    </span>
</li>

<li class="article">
  <span class="title">
    Optimal algorithms for group distributionally robust optimization and beyond
  </span>
  <span class="authors">
      Tasuku Soma, <strong>Khashayar Gatmiry</strong>, Stefanie Jegelka
  </span>
  <span class="journal-info">Preprint</span>
  <!--span class="year">2024</span-->
  <span class="links">
      <a href="https://arxiv.org/abs/2212.13669">[paper]</a>
  </span>
</li>

<li class="article">
  <span class="title">
    Information Theoretic Bounds on Optimal Worst-case Error in Binary Mixture Identification
  </span>
  <span class="authors">
      <strong>Khashayar Gatmiry</strong>, Seyed Abolfazl Motahari
  </span>
  <span class="journal-info">Preprint</span>
  <!--span class="year">2024</span-->
  <span class="links">
      <a href="https://arxiv.org/abs/1811.07307">[paper]</a>
  </span>
</li>


<h1 class="post-subsubtitle">Submodularity/Learning</h1>
<li class="article">
  <span class="title">
   The Network Visibility Problem
  </span>
  <span class="authors">
      <strong>Khashayar Gatmiry</strong>, Manuel Gomez-Rodriguez
  </span>
  <span class="journal-info">TOIS 2021</span>
  <!--span class="year">2024</span-->
  <span class="links">
      <a href="https://dl.acm.org/doi/abs/10.1145/3460475">[paper]</a>
  </span>
</li>
            <!-- <li class="article">
              <span class="title">
                Learning Motion Predictors for Smart Wheelchair using Autoregressive Sparse Gaussian Process
              </span>
              <span class="authors">Zicong Fan, Lili Meng, <strong>Tian Qi Chen</strong>, Jingchun Li, Ian M. Mitchell</span>
              <span class="journal-info">International Conference on Robotics and Automation (ICRA).</span>
              <span class="year">2018</span>
              <span class="links">
                <a href="https://arxiv.org/abs/1710.11319">arxiv</a> |
                <a href="./bibtex/motion_predictors.bib">bibtex</a>
              </span>
            </li>

            <li class="article">
              <span class="title">
                Deep kernel mean embeddings for generative modeling and feedforward style transfer
              </span>
              <span class="authors"><strong>Tian Qi Chen</strong></span>
              <span class="journal-info">MSc Thesis, University of British Columbia.</span>
              <span class="year">2017</span>
              <span class="links">
                <a href="./pdfs/msc_thesis.pdf">pdf</a> (18MB)

              </span>
            </li> -->

            <!-- <li class="article">
              <span class="title">
                Fast Patch-based Style Transfer of Arbitrary Style
                <span class="oral">(ORAL)</span>
              </span>
              <span class="authors"><strong>Tian Qi Chen</strong>, Mark Schmidt</span>
              <span class="journal-info">Workshop in Constructive Machine Learning, NIPS.</span>
              <span class="year">2016</span>
              <span class="links">
                <a href="https://arxiv.org/abs/1612.04337">arxiv</a> |
                <a href="./bibtex/fast_style.bib">bibtex</a> |
                <a href="./pdfs/fast_style_transfer_slides.pdf">slides</a> |
                <a href="./posters/styleswap_poster.pdf">poster</a> |
                <a href="https://github.com/rtqichen/style-swap/">code</a>
              </span>
            </li> -->
          </ul>
        </div>
        <!-- <span class="footnote">* denotes equal contribution, often meaning multiple authors contributed to coding and running experiments.</span> -->
      </article>
    </div>
  </main>

  <footer class="site-footer">

    <div class="wrapper">

      <div class="footer-col-wrapper">
        <div class="footer-col">
          <ul class="contact-list">
            <li>
              Khashayar Gatmiry
            </li>
            <li>Computer Science and Artificial Intelligence Laboratory, EECS, MIT</li>
            <li><a href="mailto:gatmiry@mit.edu">gatmiry@mit.edu</a></li>
          </ul>
        </div>
        <div class="footer-col">
        </div>
      </div>
    </div>


  </footer>


</body></html>
